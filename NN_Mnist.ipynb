{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mnist MultiLayerPerceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "\n",
    "def get_mnist():\n",
    "    mnist = input_data.read_data_sets('MNIST_data/', one_hot = True)\n",
    "    return mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n",
    "\n",
    "class NNModel():\n",
    "    '''\n",
    "    Logistic Regression Model\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.0001, shape = (784, 10), n_hidden = 30, n_hidden_2 = 20 , activation = tf.nn.relu):\n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.shape = shape\n",
    "        # placeholders\n",
    "        self.X = tf.placeholder(tf.float32,[None, self.shape[0]], name = 'X')\n",
    "        self.Y = tf.placeholder(tf.float32, name = 'Y')\n",
    "        #layers\n",
    "        self.fc1 = layers.fully_connected(self.X, n_hidden, activation_fn = self.activation)\n",
    "        self.fc2 = layers.fully_connected(self.fc1, n_hidden_2, activation_fn = self.activation)\n",
    "        self.Y_hat = layers.fully_connected(self.fc2, shape[1], activation_fn = None)\n",
    "        #loss and optimization\n",
    "        #self.loss = tf.reduce_mean(tf.square(self.Y - self.Y_hat, name = 'loss'))\n",
    "        # cross entropy is better for classification problem\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = self.Y, logits = self.Y_hat, name = 'loss'))\n",
    "        #self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        # prediction and accuracy\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.Y_hat, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "        # for tensorboard\n",
    "        self.loss_scalar = tf.summary.scalar('cross-entropy', self.loss)\n",
    "        self.accuracy_scalar = tf.summary.scalar('accuracy', self.accuracy)\n",
    "        self.merged_summary_op = tf.summary.merge_all()\n",
    "        # inititialization and session stuff\n",
    "        self.init_op = tf.global_variables_initializer()\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "    def fit(self, X, Y, epochs = 100, every = 10, folder = 'NNLog', batch_size = 500, init = True ):\n",
    "        '''\n",
    "        optimizes weights\n",
    "        args:\n",
    "          X: <numpy.array>, features \n",
    "          Y: <numpy.array>, labels\n",
    "          epochs: <int>, number of epochs\n",
    "          every: <int>, how often print message with Epoch and Loss values\n",
    "          folder: <string>, name of folder where to store data for TensorBoard\n",
    "          batch_size: <int>, size of features array, size of features sample\n",
    "        '''\n",
    "        total = []\n",
    "        if init:\n",
    "            self.sess.run(self.init_op)\n",
    "        summary_writer = tf.summary.FileWriter(folder, self.sess.graph)\n",
    "        x_length = len(X)\n",
    "        for i in range(epochs):\n",
    "            batch = np.random.randint(0, x_length - batch_size, 1)[0]\n",
    "            x_batch = X[batch:batch + batch_size]\n",
    "            y_batch = Y[batch:batch + batch_size]\n",
    "            _, l, a = self.sess.run([self.optimizer, self.loss, self.accuracy], feed_dict = {self.X: x_batch, self.Y: y_batch})\n",
    "            loss, accuracy = self.sess.run([self.loss_scalar, self.accuracy_scalar ], feed_dict = {self.X: x_batch, self.Y: y_batch})\n",
    "            summary_writer.add_summary(loss, i)\n",
    "            summary_writer.add_summary(accuracy)\n",
    "            total.append(l)\n",
    "            if i % every == 0:\n",
    "                print('[ {} ] Epoch {} Loss: {} Accuracy {}'.format(time.ctime(), i, l, a))\n",
    "        return total\n",
    "    \n",
    "    def predict(self, X, before_fit = False):\n",
    "        '''\n",
    "        return predicted values, \n",
    "        \n",
    "        args:\n",
    "          X: <numpy.array>, features\n",
    "          before_fit: <boolean>, change to True if you want to use before calling fit method\n",
    "        '''\n",
    "        if before_fit:\n",
    "            self.sess.run(self.init_op)\n",
    "        Y_hat = self.sess.run(self.Y_hat, feed_dict = {self.X: X})\n",
    "        return Y_hat\n",
    "    \n",
    "    def show(self, values):\n",
    "        '''\n",
    "        plot graph\n",
    "        \n",
    "        args:\n",
    "          values: <list>, list or array of values to be plotted\n",
    "        '''\n",
    "        plt.plot(values)\n",
    "        plt.show()\n",
    "    \n",
    "    def close_session(self):\n",
    "        '''\n",
    "        closes tensorflow session\n",
    "        '''\n",
    "        self.sess.close()\n",
    "        return True\n",
    "    \n",
    "    def close_and_reset(self):\n",
    "        '''\n",
    "        closes tensorflow session, clears the default graph stack and resets the global default graph.\n",
    "        '''\n",
    "        self.sess.close()\n",
    "        tf.reset_default_graph()\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = get_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Wed Dec 26 12:20:21 2018 ] Epoch 0 Loss: 2.3153092861175537 Accuracy 0.12080000340938568\n",
      "[ Wed Dec 26 12:20:37 2018 ] Epoch 500 Loss: 0.6353247165679932 Accuracy 0.8568000197410583\n",
      "[ Wed Dec 26 12:20:54 2018 ] Epoch 1000 Loss: 0.35971659421920776 Accuracy 0.9020000100135803\n",
      "[ Wed Dec 26 12:21:10 2018 ] Epoch 1500 Loss: 0.3106103837490082 Accuracy 0.9115999937057495\n",
      "[ Wed Dec 26 12:21:27 2018 ] Epoch 2000 Loss: 0.2133604735136032 Accuracy 0.9395999908447266\n",
      "[ Wed Dec 26 12:21:43 2018 ] Epoch 2500 Loss: 0.22712615132331848 Accuracy 0.9359999895095825\n",
      "[ Wed Dec 26 12:22:00 2018 ] Epoch 3000 Loss: 0.2420654296875 Accuracy 0.9323999881744385\n",
      "[ Wed Dec 26 12:22:17 2018 ] Epoch 3500 Loss: 0.19117936491966248 Accuracy 0.946399986743927\n",
      "[ Wed Dec 26 12:22:33 2018 ] Epoch 4000 Loss: 0.170494943857193 Accuracy 0.9488000273704529\n",
      "[ Wed Dec 26 12:22:50 2018 ] Epoch 4500 Loss: 0.16485324501991272 Accuracy 0.9527999758720398\n",
      "[ Wed Dec 26 12:23:06 2018 ] Epoch 5000 Loss: 0.14597132802009583 Accuracy 0.9592000246047974\n",
      "[ Wed Dec 26 12:23:23 2018 ] Epoch 5500 Loss: 0.1408269703388214 Accuracy 0.9575999975204468\n",
      "[ Wed Dec 26 12:23:40 2018 ] Epoch 6000 Loss: 0.1339489370584488 Accuracy 0.9624000191688538\n",
      "[ Wed Dec 26 12:23:56 2018 ] Epoch 6500 Loss: 0.13076987862586975 Accuracy 0.9616000056266785\n",
      "[ Wed Dec 26 12:24:13 2018 ] Epoch 7000 Loss: 0.11370772868394852 Accuracy 0.9639999866485596\n",
      "[ Wed Dec 26 12:24:30 2018 ] Epoch 7500 Loss: 0.10079260170459747 Accuracy 0.9711999893188477\n",
      "[ Wed Dec 26 12:24:47 2018 ] Epoch 8000 Loss: 0.12612533569335938 Accuracy 0.9684000015258789\n",
      "[ Wed Dec 26 12:25:03 2018 ] Epoch 8500 Loss: 0.0940936878323555 Accuracy 0.97079998254776\n",
      "[ Wed Dec 26 12:25:20 2018 ] Epoch 9000 Loss: 0.10532677173614502 Accuracy 0.9715999960899353\n",
      "[ Wed Dec 26 12:25:37 2018 ] Epoch 9500 Loss: 0.10078394412994385 Accuracy 0.9732000231742859\n"
     ]
    }
   ],
   "source": [
    "nn = NNModel()\n",
    "losses = nn.fit(X_train, Y_train, epochs = 10000, every = 500, batch_size = 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

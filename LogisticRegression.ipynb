{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST logistic regression ( Code is heavily inspired by book TensorFlow 1.x Deep Learning Cookbook )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "def get_mnist():\n",
    "    mnist = input_data.read_data_sets('MNIST_data/', one_hot = True)\n",
    "    return mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n",
    "    \n",
    "class LogisticRegressionModel():\n",
    "    '''\n",
    "    Logistic Regression Model\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.01, shape = (784, 10)):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.shape = shape\n",
    "        # placeholders\n",
    "        self.X = tf.placeholder(tf.float32, name = 'X')\n",
    "        self.Y = tf.placeholder(tf.float32, name = 'Y')\n",
    "        #variables\n",
    "        self.w = tf.Variable(tf.random_normal([self.shape[0],self.shape[1]]), name = 'w')\n",
    "        self.b = tf.Variable(tf.zeros([self.shape[1]]), name = 'bias')\n",
    "        #predicted labels\n",
    "        self.Y_hat = tf.nn.softmax(tf.matmul(self.X, self.w)+ self.b)\n",
    "        #loss and optimization\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.Y, logits = self.Y_hat, name = 'loss'))\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        # prediction and accuracy\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.Y_hat, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "        # for tensorboard\n",
    "        self.w_histogram = tf.summary.histogram('weights', self.w)\n",
    "        self.b_histrogram = tf.summary.histogram('biases', self.b)\n",
    "        self.loss_scalar = tf.summary.scalar('cross-entropy', self.loss)\n",
    "        self.accuracy_scalar = tf.summary.scalar('accuracy', self.accuracy)\n",
    "        self.merged_summary_op = tf.summary.merge_all()\n",
    "        # inititialization and session stuff\n",
    "        self.init_op = tf.global_variables_initializer()\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "    def fit(self, X, Y, epochs = 100, every = 10, folder = 'LogisticRegression', batch_size = 500, init = True ):\n",
    "        '''\n",
    "        optimizes weights\n",
    "        args:\n",
    "          X: <numpy.array>, features \n",
    "          Y: <numpy.array>, labels\n",
    "          epochs: <int>, number of epochs\n",
    "          every: <int>, how often print message with Epoch and Loss values\n",
    "          folder: <string>, name of folder where to store data for TensorBoard\n",
    "          batch_size: <int>, size of features array, size of features sample\n",
    "        '''\n",
    "        total = []\n",
    "        if init:\n",
    "            self.sess.run(self.init_op)\n",
    "        summary_writer = tf.summary.FileWriter(folder, self.sess.graph)\n",
    "        x_length = len(X)\n",
    "        for i in range(epochs):\n",
    "            batch = np.random.randint(0, x_length - batch_size, 1)[0]\n",
    "            x_batch = X[batch:batch + batch_size]\n",
    "            y_batch = Y[batch:batch + batch_size]\n",
    "            _, l, a = self.sess.run([self.optimizer, self.loss, self.accuracy], feed_dict = {self.X: x_batch, self.Y: y_batch})\n",
    "            w = self.sess.run(self.w_histogram)\n",
    "            b = self.sess.run(self.b_histrogram)\n",
    "            loss, accuracy = self.sess.run([self.loss_scalar, self.accuracy_scalar ], feed_dict = {self.X: x_batch, self.Y: y_batch})\n",
    "            summary_writer.add_summary(w, i)\n",
    "            summary_writer.add_summary(b, i)\n",
    "            summary_writer.add_summary(loss, i)\n",
    "            summary_writer.add_summary(accuracy)\n",
    "            total.append(l)\n",
    "            if i % every == 0:\n",
    "                print('Epoch {} Loss: {} Accuracy {}'.format(i, l, a))\n",
    "        weights = self.sess.run(self.w)\n",
    "        return total, weights\n",
    "    \n",
    "    def predict(self, X, before_fit = False):\n",
    "        '''\n",
    "        return predicted values, \n",
    "        \n",
    "        args:\n",
    "          X: <numpy.array>, features\n",
    "          before_fit: <boolean>, change to True if you want to use before calling fit method\n",
    "        '''\n",
    "        if before_fit:\n",
    "            self.sess.run(self.init_op)\n",
    "        Y_hat = self.sess.run(self.Y_hat, feed_dict = {self.X: X})\n",
    "        return Y_hat\n",
    "    \n",
    "    def show(self, values):\n",
    "        '''\n",
    "        plot graph\n",
    "        \n",
    "        args:\n",
    "          values: <list>, list or array of values to be plotted\n",
    "        '''\n",
    "        plt.plot(values)\n",
    "        plt.show()\n",
    "    \n",
    "    def close_session(self):\n",
    "        '''\n",
    "        closes tensorflow session\n",
    "        '''\n",
    "        self.sess.close()\n",
    "        return True\n",
    "    \n",
    "    def close_and_reset(self):\n",
    "        '''\n",
    "        closes tensorflow session, clears the default graph stack and resets the global default graph.\n",
    "        '''\n",
    "        self.sess.close()\n",
    "        tf.reset_default_graph()\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = get_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 2.345693588256836 Accuracy 0.10333333164453506\n",
      "Epoch 10000 Loss: 2.273240804672241 Accuracy 0.17866666615009308\n",
      "Epoch 20000 Loss: 2.2020998001098633 Accuracy 0.2526666522026062\n",
      "Epoch 30000 Loss: 2.140986919403076 Accuracy 0.3166666626930237\n",
      "Epoch 40000 Loss: 2.141397714614868 Accuracy 0.3160000145435333\n",
      "Epoch 50000 Loss: 2.1065547466278076 Accuracy 0.34933334589004517\n",
      "Epoch 60000 Loss: 2.1046860218048096 Accuracy 0.3526666760444641\n",
      "Epoch 70000 Loss: 2.1148412227630615 Accuracy 0.3440000116825104\n",
      "Epoch 80000 Loss: 2.1084766387939453 Accuracy 0.3479999899864197\n",
      "Epoch 90000 Loss: 2.1058738231658936 Accuracy 0.3526666760444641\n",
      "Epoch 100000 Loss: 2.0892679691314697 Accuracy 0.36666667461395264\n",
      "Epoch 110000 Loss: 2.0803966522216797 Accuracy 0.3766666650772095\n",
      "Epoch 120000 Loss: 2.08878231048584 Accuracy 0.36800000071525574\n",
      "Epoch 130000 Loss: 2.0986998081207275 Accuracy 0.36000001430511475\n",
      "Epoch 140000 Loss: 2.0744729042053223 Accuracy 0.3813333213329315\n",
      "Epoch 150000 Loss: 2.0742576122283936 Accuracy 0.38333332538604736\n",
      "Epoch 160000 Loss: 2.073190450668335 Accuracy 0.38333332538604736\n",
      "Epoch 170000 Loss: 2.0211803913116455 Accuracy 0.43799999356269836\n",
      "Epoch 180000 Loss: 2.0508975982666016 Accuracy 0.4073333442211151\n",
      "Epoch 190000 Loss: 2.056039810180664 Accuracy 0.4020000100135803\n",
      "Epoch 200000 Loss: 2.0361931324005127 Accuracy 0.42133334279060364\n",
      "Epoch 210000 Loss: 2.046668529510498 Accuracy 0.41066667437553406\n",
      "Epoch 220000 Loss: 2.043733596801758 Accuracy 0.4113333225250244\n",
      "Epoch 230000 Loss: 2.0352985858917236 Accuracy 0.4233333468437195\n",
      "Epoch 240000 Loss: 2.040628671646118 Accuracy 0.4166666567325592\n",
      "Epoch 250000 Loss: 1.9955564737319946 Accuracy 0.46399998664855957\n",
      "Epoch 260000 Loss: 2.0197396278381348 Accuracy 0.4386666715145111\n",
      "Epoch 270000 Loss: 2.0385656356811523 Accuracy 0.4206666648387909\n"
     ]
    }
   ],
   "source": [
    "log = LogisticRegressionModel()\n",
    "losses, weights = log.fit(X_train, Y_train, epochs = 500000, every = 10000, batch_size = 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "below short numpy code as calculate accuracy for this model\n",
    "'''\n",
    "predict = log.predict(X_test[:20]) #predicted values\n",
    "predicted = [np.argmax(item) for item in predict] # change from one_hot encoding to numerical values\n",
    "target = [np.argmax(item) for item in Y_test[:20]] # change from one_hot encoding to numerical values\n",
    "comparison = np.equal(predicted, target).astype(int) # returns boolean comparison converted to integers (0,1)\n",
    "sum(comparison)/len(comparison) # sum of ones divided by length of array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [np.argmax(item) for item in Y_test[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.close_and_reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison = np.equal(predicted, target).astype(int)\n",
    "sum(comparison)/len(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

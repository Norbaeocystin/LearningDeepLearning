'''
Code to improve my bot: https://github.com/Norbaeocystin/FxBot
Date: December 2018
Author: Rastislav Baran
Usage: to improve profitable dates generated by bot

Here are few potential problems to make analysis less profitable trades than unprofitable

Example of usage:

X, Y = get_structured_data(3600)

rnn = NNModel()
rnn.fit(X,Y, generator = True)

x, y = generate_random_dataset(X,Y)
predicted = rnn.sess.run(rnn.predictions, feed_dict = {rnn.X: x} )
#for all dataseet
X_normalized = normalize_maxmin(X)
predicted = rnn.sess.run(rnn.predictions, feed_dict = {rnn.X: X} )
'''
import datetime
import logging
import numpy as np
import pandas as pd
import pymongo
from pymongo import MongoClient
from sklearn.preprocessing import MinMaxScaler
from sklearn.utils import shuffle
import tensorflow as tf
import time

# Connectio URI can be in shape mongodb://<username>:<password>@<ip>:<port>/<authenticationDatabase>')
CONNECTION = MongoClient('localhost')
db = CONNECTION.Bot
FxData = db.FxData

logging.basicConfig(level=logging.DEBUG, format = '%(asctime)s %(name)s %(levelname)s %(message)s')
logger = logging.getLogger(__name__)



def get_info_about_trades():
    '''
    returns: <dict>, profitable trades and unprofitable with sum profits and losses as dict
    '''
    return FxData.aggregate([{'$facet':{'loss':[{ '$match': { 'order': { '$exists': True }, 'profit' :{'$lt':12} }},{'$count': "No"}], 
                                        'profitable':[{ '$match': { 'order': { '$exists': True }, 'profit' :{'$gt':12} }},{'$count': "No"}], 
                                        'PL':[{ '$match': { 'order': { '$exists': True } }},{ '$group': { '_id' : None, 'sum' : { '$sum': "$profit" }}}]}
                             }]).next()

def get_trades():
    '''
    function which retuns list of dictionaries with data about closed trades
    returns: <list>, list of dictionaries with data about closed trades
    '''
    return list(FxData.aggregate([{ '$match': { 'order': { '$exists': True }}}]))

def get_structured_data(time = 100, limit = 0):
    '''
    Function to get structured data from MongoDB, please it can take time ...
    returns: <tuple>, tuple: <numpy.array>, <numpy.array> or if you want: features, labels
        
    args:
        time: <int>, how long features ( in this case timeserie) will be long in seconds
        limit: <int>, positive integer to limit number of trades if necessary
    '''
    trades = get_trades()
    if limit != 0 and limit > 0:
        trades = trades[:limit]
    #generate labels
    profits = [item.get('profit') for item in trades]
    logger.debug('Gets {} trades'.format(len(profits)))
    #convert list profits and losses to binary
    labels = np.array([[(0 if item < 0 else 1)] for item in profits]).astype(np.uint8)
    # features
    times = [item.get('Time') for item in trades]
    features = []
    logger.debug('Labels done. Working on features')
    # fetching data for features and structuring them
    for ind, item in enumerate(times):
        # lower or equal time that in item, only price to get, reversed it by sorting, limit size of data and reversed it again 
        x = list(FxData.find({'Time':{'$lte':item}},{"Price":1, '_id':0}).sort('Time', pymongo.DESCENDING).limit(time))[::-1]
        if len(x) != time:
            logger.debug('Data are shorter for item {}'.format(ind))
            break
        features.append([item.get('Price') for item in x])
        if ind % 10 == 0:
            logger.debug('Done structuring {} trades'.format(ind))
    features = np.array(features).astype(np.float32)
    logger.debug('Your data are prepared')
    return features, labels

def normalize_on_average(X):
    '''
    calculate average and return for each row: row - mean
    return: <numpy.ndarray>, with values between 0 and 1
    
    args:
        X: <numpy.ndarray>, features, labels whatever
    '''
    res = []
    for item in X:
        item = item - np.mean(item)
        res.append(item)
    return np.array(res)

def normalize_maxmin(X):
    '''
    MaxMinNormalizer
    return: <numpy.ndarray>, with values between 0 and 1
    
    args:
        X: <numpy.ndarray>, features, labels whatever
    '''
    return MinMaxScaler().fit_transform(X)

def generate_random_dataset(X, Y, ratio_of_profit = 0.7):
    '''
    one solution how to generate dataset with ration 50 % of profitable trades and loss trades
    return: <list>, list of shuffled features and labels as numpy.ndarray
    
    args:
        X: <numpy.ndarray>, features
        Y: <numpy.ndarray>, labels with shape (x, 1)
        ratio_of_profit: <float>, between 0 to 1, how many of profitable trades to use
    '''
    #returns indices of profitable trades
    indices_profitable = np.argwhere(Y == 1)[:,0]
    indices_profitable = np.random.choice(indices_profitable, size =  int(indices_profitable.shape[0]* ratio_of_profit))
    #returns profitable trades
    profitable_features = np.take(X,[indices_profitable], axis =0)[0]
    #now for loss trades
    indices_loss = np.argwhere(Y == 0)[:,0]
    indices_loss = np.random.choice(indices_loss, size =  int(indices_profitable.shape[0]* ratio_of_profit ))
    #returns profitable trades
    loss_features = np.take(X,[indices_loss], axis =0)[0]
    #concatenate
    features = np.concatenate((profitable_features, loss_features), axis = 0)
    profits = np.array([[1]] * profitable_features.shape[0])
    losses = np.array([[0]] * loss_features.shape[0])
    labels = np.concatenate([profits, losses], axis = 0)
    #shuffle dataset
    features = normalize_maxmin(features)
    return shuffle(features, labels)


class NNModel:
    '''
    simple class few layers of Neurons to predict about orders made
    '''
    def __init__(self, input_dim = 3600, output_dim = 1, learning_rate = 0.00001, dropout = 0.5):
        self.X  = tf.placeholder(tf.float32, shape = [None, input_dim], name = 'X')
        self.Y = tf.placeholder(tf.float32, shape = [None, output_dim], name = 'Y')
        self.logdir = 'logs/RNN_with_summaries'
        self.learning_rate = learning_rate
        self.dropout = dropout
        self.keep_prob = tf.placeholder_with_default(1.0, shape=())
        #NN
        '''
        posibility to add conv1d filters
        
        #input
        i = tf.constant([1, 0, 2, 3, 0, 1, 1,0], dtype=tf.float32, name='i')
        #kernel
        k = tf.constant([0, 1, 0], dtype=tf.float32, name='k')
        #reshaping
        data   = tf.reshape(i, [1, int(i.shape[0]), 1], name='data')
        kernel = tf.reshape(k, [int(k.shape[0]), 1, 1], name='kernel')
        #squeezing is neccessary to remove one dimension
        res = tf.squeeze(tf.nn.conv1d(data, kernel, 2, 'SAME'))
        '''
        self.fc1 = tf.contrib.layers.fully_connected(self.X, 2048, activation_fn = tf.nn.relu)
        self.d1 = tf.nn.dropout(self.fc1, self.keep_prob)
        self.fc2 = tf.contrib.layers.fully_connected(self.d1, 1024, activation_fn = tf.nn.relu) 
        self.fc3 = tf.contrib.layers.fully_connected(self.fc2, 1024, activation_fn = tf.nn.relu) 
        self.fc4 = tf.contrib.layers.fully_connected(self.fc3, 1024, activation_fn = tf.nn.relu) 
        self.d2 = tf.nn.dropout(self.fc4, self.keep_prob)
        self.fc5 = tf.contrib.layers.fully_connected(self.d2, 32, activation_fn = tf.nn.relu) 
        self.prediction = tf.layers.dense(self.fc5, 1, activation = tf.sigmoid)
        self.cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = self.prediction, labels = self.Y))
        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.cost)
        self.predicted_class = tf.greater(self.prediction,0.5)
        self.correct = tf.equal(self.predicted_class, tf.equal(self.Y,1.0))
        self.accuracy = tf.reduce_mean( tf.cast(self.correct, 'float') )
        self.init_op = tf.global_variables_initializer()
        self.config = tf.ConfigProto(device_count = {'GPU': 0})
        self.sess = tf.Session(config = self.config)
        self.accuracy_scalar = tf.summary.scalar('accuracy', self.accuracy)
        self.saver = tf.train.Saver()
    
    def fit(self, X, Y, epochs = 100000,every = 1000, generator = False, log = 'Forex_NN_log', init = True):
        '''
        optimizes weights
        args:
          X: <numpy.array>, features 
          Y: <numpy.array>, labels
          epochs: <int>, number of epochs
          every: <int>, how often print message with Epoch and Loss values
          folder: <string>, name of folder where to store data for TensorBoard
          batch_size: <int>, size of features array, size of features sample
          init: <boolean>, if True will initialize variables if false will not, set init False if after restoring saved model
        '''
        if init:
            self.sess.run(self.init_op)
        logger.debug('Initializing')
        summary_writer = tf.summary.FileWriter(log, self.sess.graph)
        for i in range(epochs):
            if generator:
                x, y = generate_random_dataset(X,Y)
                _, l,a, acc = self.sess.run([self.optimizer, self.cost,self.accuracy, self.accuracy_scalar], 
                                            feed_dict = {self.X: x, self.Y: y, self.keep_prob: self.dropout})
                summary_writer.add_summary(acc, i)
                if i % every == 0:
                    self.save(global_step = i)
                    print('[ {} ] Epoch {} Loss: {:.7f} accuracy {}'.format(time.ctime(), i, l,a)) 
                    
    def predict(self, X):
        '''
        predict values
        '''
        y_hat = self.sess.run(self.prediction, feed_dict = {self.X: X})
        return y_hat
    
    def compare(self,prediction, target):
        '''
        compare predicted values with targeted values
        returns: <dict>, {'Profitable Trades': <int>, 'Lost_trades': <int>}
        
        args:
            prediction: <numpy.ndarray>
            target: <numpy.ndarray>
        '''
        profitable = np.argwhere(target == 1)[:,0]
        calculated_profitable = np.argwhere(np.round(prediction) == 1)[:,0]
        set_profitable = set(profitable)
        really_profitable_trades = set_profitable.intersection(calculated_profitable)
        lost = len(calculated_profitable) - len(really_profitable_trades)
        return {'Profitable Trades': len(really_profitable_trades), 'Lost_trades': lost}
        
   
    def variable_summaries(self, var):
        '''
        
        '''
        with tf.name_scope('summaries'):
            mean = tf.reduce_mean(var)
            tf.summary.scalar('mean', mean)
            with tf.name_scope('stddev'):
                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
            tf.summary.scalar('stddev', stddev)
            tf.summary.scalar('max', tf.reduce_max(var))
            tf.summary.scalar('min', tf.reduce_min(var))
            tf.summary.histogram('histogram', var)
            
    def save(self, path  ='./Model/model', global_step = 0):
        '''
        save model
        args:
            path: <string>, path with name where will be model stored

        path can be:
        path = './Model/RNN_{}'.format(time.ctime().replace(' ','_'))
        '''
        self.saver.save(self.sess, path, global_step = global_step)
        
    def restore(self, path = 'Model/model-20000'):
        '''
        restores saved model
        '''
        self.saver.restore(self.sess, path)
    
    def close(self):
        '''
        close session
        '''
        self.sess.close()
        
    def close_and_reset(self):
        '''
        closes tensorflow session, clears the default graph stack and resets the global default graph
        '''
        self.sess.close()
        tf.reset_default_graph()

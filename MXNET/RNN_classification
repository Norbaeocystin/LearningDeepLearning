"""
example of usage:

#sample data
data = "1\tStrong like bull. This wants to explode up, and it will.\n0\tthis is such a bull trap\n0\tI am not buying\n1\tI am buying\n0\tI am selling\n1\tI am not selling"
lines = data.split('\n')
labels = [item.split('\t')[0] for item in lines] * 200
labels = [float(label) for label in labels]
sentences = [item.split('\t')[1] for item in lines] * 200
sentences = [[ord(char) for char in sentence] for sentence in sentences]
max_length = max([len(item) for item in sentences])
padded_sentences = [ ([0] * (max_length - len(item))) + item for item in sentences]
#char embedding
"""
embeddings = nd.zeros(shape=(128,128))
for i in range(0,128): 
    embeddings[i][i] = 1.
"""
#binary char embedding
def generate_zeros_and_ones(integer, bits = 7):
    res = []
    bit = 1
    for i in range(bits):
        res.append(bit)
        bit *= 2
    return [1.0 if integer & item else 0.0 for item in res[::-1]]
#it encodes char ( with value 0 - 127) to array of ones and zeros
# we can look on it as bit encoding instead of one hot encoding of char - in this case embedding would have size 128 x 128
embeddings = []
for i in range(128):
    bits = generate_zeros_and_ones(i)
    embeddings.append(bits)
embeddings = nd.array(embeddings)
X = nd.array(padded_sentences)
Y = nd.array(labels).reshape(-1,1)

print(time.ctime())
x = X.copyto(mx.gpu(0))
y = Y.copyto(mx.gpu(0))
start = time.time()
rn = SentimentRNN( 128, 7, 128, 2, mx.gpu(0), embeddings)
rn.fit(x, y, epochs = 11, batch = 100, learning_rate = 0.001, every = 1)
print('11 Epochs on GPU 0 takes: {} with binary embedding'.format(time.time() - start))

"""

import mxnet as mx
from mxnet import gluon, init, nd, autograd
from mxnet.gluon import nn, rnn
import time

class BiRNN(nn.Block):
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, ctx, **kwargs):
        super(BiRNN, self).__init__(**kwargs)
        self.ctx = ctx
        self.embedding = nn.Embedding(vocab_size, embed_size)
        # Set Bidirectional to True to get a bidirectional recurrent neural
        # network
        self.encoder = rnn.LSTM(num_hiddens, num_layers=num_layers,bidirectional=True, input_size=embed_size)
        self.decoder = nn.Dense(2, activation="sigmoid")
        self.ctx = ctx

    def forward(self, inputs):
        # The shape of inputs is (batch size, number of words). Because LSTM
        # needs to use sequence as the first dimension, the input is
        # transformed and the word feature is then extracted. The output shape
        # is (number of words, batch size, word vector dimension).
        embeddings = self.embedding(inputs.T)
        # Since the input (embeddings) is the only argument passed into
        # rnn.LSTM, it only returns the hidden states of the last hidden layer
        # at different time step (outputs). The shape of outputs is
        # (number of words, batch size, 2 * number of hidden units).
        outputs = self.encoder(embeddings)
        # Concatenate the hidden states of the initial time step and final
        # time step to use as the input of the fully connected layer. Its
        # shape is (batch size, 4 * number of hidden units)
        encoding = nd.concat(outputs[0], outputs[-1])
        outs = self.softmax(self.decoder(encoding))
        return outs
    
    def softmax(self, X):
        X_exp = nd.exp(X)
        partition = X_exp.sum(axis=1, keepdims=True)
        return X_exp / partition
    
class SentimentRNN(object):
    
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, ctx, embeds):
        self.ctx = ctx
        self.net = BiRNN(vocab_size, embed_size, num_hiddens, num_layers, self.ctx)
        self.net.initialize(init.Xavier(), ctx= self.ctx)
        self.embeds = embeds
        self.net.embedding.weight.set_data(self.embeds)
        self.net.embedding.weight.reset_ctx(self.ctx)
        self.net.embedding.collect_params().setattr('grad_req', 'null')
        
    def accuracy(self, y_hat, y):
        return ((y_hat.argmax(axis = 1).reshape(-1,1) == y).sum()/y.shape[0]).asscalar()
        
    def fit(self, X,Y, epochs = 101, batch = 100, learning_rate = 0.1, every = 10):
        self.X = X
        self.Y = Y
        self.indices = nd.arange(Y.shape[0])
        self.trainer = gluon.Trainer(self.net.collect_params(), 'adam', {'learning_rate': learning_rate})
        #self.loss = gluon.loss.SoftmaxCrossEntropyLoss()
        def cross_entropy(y_hat, y):
            return - nd.pick(y_hat, y).log()
        self.batch = batch
        if self.batch > self.Y.shape[0]:
            self.batch = self.Y.shape[0]
        self.epochs = epochs
        dataset = gluon.data.ArrayDataset(X,Y)
        data_iter =  gluon.data.DataLoader(dataset, batch, shuffle = True)
        for epoch in range(1, epochs + 1):
            for x, y in data_iter:
                x.as_in_context(self.ctx)
                y.as_in_context(self.ctx)
                with autograd.record():
                    l = cross_entropy(self.net(x), y)
                l.backward()
                self.trainer.step(batch, ignore_stale_grad=True)
            if epoch % every == 0:
                loss = cross_entropy(self.net(x), y).mean().asnumpy()
                acc = self.accuracy(self.net(x), y)
                print('{} epoch {} loss: {}, acc {}'.format(time.ctime(), epoch, loss, acc))
                del loss
                del acc
            del x
            del y
        
    def data_iter(self):
        while True:
            nd.shuffle(self.indices, out = self.indices)
            inds = nd.array(self.indices[:self.batch], ctx = self.ctx)
            yield self.X.take(inds), self.Y.take(inds)      
        
    def predict(self,X):
        label = self.net(X)
        return label
		
